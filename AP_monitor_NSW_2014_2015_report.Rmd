---
title: "Air pollution data 2014-2015 processing report"
author: "Ivan C. Hanigan"
output: 
    html_document:
        toc: true
---

# Aim
- clean the data
- standardise to same format as biomass smoke event database
- this standardisation is required so that these can be imported to the database and so that the imputed network average and extreme events can be computed downstream in the biomass_smoke_events_db project

# Data methods

## Step 1: Visit the website and download the data

Visit the website [http://www.environment.nsw.gov.au/AQMS/search.htm]() then:

1. select data category = 'daily averages/max/min), 
2. select 'download', 
3. sites (do this for all regions, or as required - more selections run the risk of hitting the maximum size limit and download will fail)
4. Hit 'load data', wait, then click link and save the file to a local directory such as '/data_provided'   

Below is a screenshot of this procedure:

![figures_and_tables/ap_nsw_download.png](figures_and_tables/ap_nsw_download.png)

## Step 2: read in to R and clean

1. open the xls file in a spreadsheet program like Excel or LibreOffice
2. use the 'save as' function to export this to a CSV file with the same name
3. for each file in the download log we will conduct the same procedure so create a loop

```{r, eval = F}
indir <- "data_provided"
dir(indir)
tab
infile <- tab$filename[1]
infile
outdir <- "data_derived"
outfile <- "air_pollution_sydney_2008_2014.csv"

# load
dat  <- read.csv(file.path(indir, infile), skip = 2)
str(dat)

```

# Conclusions
